{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b909b8b3-9199-44c7-9e2a-4bc46afca8e4",
   "metadata": {},
   "source": [
    "ASSIGNMENT:REGRESSION-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658056ff-1c19-4199-8f83-bd1dc5f4fb67",
   "metadata": {},
   "source": [
    "1.  Explain the concept of R-squared in linear regression models. How is it calculated, and what does it \n",
    "represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f389248d-b1ac-48af-806f-ed4c2edb9315",
   "metadata": {},
   "source": [
    "R-squared is a statistical measure that represents the proportion of variation in the dependent variable that is explained by the independent variable(s) in a linear regression model. It is calculated using the following formulas:\n",
    "\n",
    "Formula 1:\n",
    "\n",
    "R-squared = 1 - (RSS / TSS)\n",
    "\n",
    "where,\n",
    "\n",
    "RSS (Residual Sum of Squares) = ∑(Y_actual - Y_predicted)²\n",
    "TSS (Total Sum of Squares) = ∑(Y_actual - Y_mean)²\n",
    "\n",
    "Formula 1 calculates R-squared by taking the ratio of the explained variance (TSS - RSS) to the total variance (TSS).\n",
    "\n",
    "Formula 2:\n",
    "\n",
    "R-squared = (SSR / SST)\n",
    "\n",
    "where,\n",
    "\n",
    "SSR (Sum of Squares Regression) = ∑(Y_predicted - Y_mean)²\n",
    "SST (Total Sum of Squares) = ∑(Y_actual - Y_mean)²\n",
    "\n",
    "Formula 2 calculates R-squared by taking the ratio of the sum of squares regression (SSR) to the total sum of squares (SST)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0534223c-3ea3-48e4-90c4-f196fbd40467",
   "metadata": {},
   "source": [
    "2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f649ee-bbd9-47af-a896-e665c4ab05a8",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a statistical measure used to assess the goodness of fit of a linear regression model that takes into account the number of independent variables included in the model. It is a modification of the regular R-squared that penalizes the model for including unnecessary independent variables, which can lead to overfitting of the data.\n",
    "\n",
    "Adjusted R-squared is calculated using the following formula:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R²) * (n - 1) / (n - p - 1)]\n",
    "\n",
    "where,\n",
    "\n",
    "R² = the regular R-squared value\n",
    "n = the number of data points\n",
    "p = the number of independent variables in the model\n",
    "\n",
    "The adjusted R-squared value ranges from 0 to 1, with a higher value indicating a better fit of the model. Compared to the regular R-squared, the adjusted R-squared is generally a more accurate measure of the goodness of fit of a model when there are multiple independent variables.\n",
    "\n",
    "The difference between the regular R-squared and the adjusted R-squared is that the latter takes into account the number of independent variables in the model. As the number of independent variables increases, the regular R-squared value may also increase, even if the additional variables do not contribute significantly to the model. The adjusted R-squared value, on the other hand, penalizes the model for including unnecessary variables, leading to a more conservative estimate of the model's goodness of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca59ba-6ae7-4a93-9d33-7021d6870852",
   "metadata": {},
   "source": [
    "3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6898d9d5-805a-449b-8a0a-f1785dcae702",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate to use when there are multiple independent variables in the linear regression model. The regular R-squared value can increase even if the additional independent variables do not contribute significantly to the model. Therefore, the adjusted R-squared value provides a more accurate measure of the goodness of fit of the model by penalizing the model for including unnecessary independent variables.\n",
    "\n",
    "Adjusted R-squared is especially useful in situations where there are many independent variables in the model, or when there is a risk of overfitting the data. Overfitting occurs when a model fits the training data too closely, resulting in a high R-squared value but poor performance on new data.\n",
    "\n",
    "Adjusted R-squared also helps in comparing the performance of different regression models that have a different number of independent variables. For example, if two models have similar regular R-squared values but one has fewer independent variables, the adjusted R-squared value will likely be higher for the model with fewer independent variables. In such cases, the adjusted R-squared value can help to identify the model that is a better fit for the data.\n",
    "\n",
    "Overall, adjusted R-squared should be used in conjunction with other statistical measures and tests to evaluate the performance of a linear regression model, such as hypothesis testing, confidence intervals, and residual plots.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3607a6d-4261-4923-a752-cc0de4202d67",
   "metadata": {},
   "source": [
    "4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics \n",
    "calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211e9d1a-7ae4-4bcf-8b6f-8fbbc44719ce",
   "metadata": {},
   "source": [
    "Root Mean Squared Error (RMSE):\n",
    "RMSE is a measure of the standard deviation of the errors of the predicted values. It is calculated as the square root of the average of the squared differences between the predicted and actual values. The formula for RMSE is:\n",
    "\n",
    "RMSE = sqrt[1/n * sum(i=1 to n) of (Yi - Ŷi)^2]\n",
    "\n",
    "where,\n",
    "n = number of observations\n",
    "Yi = actual value of the dependent variable for observation i\n",
    "Ŷi = predicted value of the dependent variable for observation i\n",
    "\n",
    "RMSE is sensitive to outliers in the data and is useful when large errors are particularly undesirable.\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "MSE is a measure of the average of the squared differences between the predicted and actual values. The formula for MSE is:\n",
    "\n",
    "MSE = 1/n * sum(i=1 to n) of (Yi - Ŷi)^2\n",
    "\n",
    "where,\n",
    "n = number of observations\n",
    "Yi = actual value of the dependent variable for observation i\n",
    "Ŷi = predicted value of the dependent variable for observation i\n",
    "\n",
    "MSE is also sensitive to outliers in the data and is useful when large errors are particularly undesirable.\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "MAE is a measure of the average of the absolute differences between the predicted and actual values. The formula for MAE is:\n",
    "\n",
    "MAE = 1/n * sum(i=1 to n) of |Yi - Ŷi|\n",
    "\n",
    "where,\n",
    "n = number of observations\n",
    "Yi = actual value of the dependent variable for observation i\n",
    "Ŷi = predicted value of the dependent variable for observation i\n",
    "\n",
    "MAE is less sensitive to outliers than RMSE and MSE, and is useful when small errors are more important than large errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b24366-0823-4151-b338-6097388aed1a",
   "metadata": {},
   "source": [
    "5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in \n",
    "regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e9b468-872c-484f-9e28-04e0947a7316",
   "metadata": {},
   "source": [
    "Root Mean Squared Error (RMSE):\n",
    "The RMSE is a commonly used evaluation metric in regression analysis, which measures the standard deviation of the errors made by the model's predictions. It is calculated by taking the square root of the average of the squared differences between the predicted and actual values of the dependent variable. RMSE is a more sensitive metric to outliers, compared to MSE and MAE. Therefore, it is particularly useful when larger errors are particularly undesirable. However, RMSE is also sensitive to the scale of the dependent variable and may be challenging to compare across models or studies that use different scales.\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "MSE is another commonly used evaluation metric in regression analysis, which measures the average of the squared differences between the predicted and actual values of the dependent variable. It is calculated by taking the average of the squared differences between the predicted and actual values of the dependent variable. Like RMSE, MSE is sensitive to outliers, but it is less sensitive to outliers compared to RMSE. The main advantage of MSE is that it is mathematically convenient and useful in some cases. However, as with RMSE, it is sensitive to the scale of the dependent variable.\n",
    "\n",
    "Mean Absolute Error (MAE):\n",
    "MAE is a third commonly used evaluation metric in regression analysis, which measures the average of the absolute differences between the predicted and actual values of the dependent variable. It is calculated by taking the average of the absolute differences between the predicted and actual values of the dependent variable. Compared to RMSE and MSE, MAE is less sensitive to outliers, and it provides information on the directionality of the error. This means it can be useful when small errors are more important than large errors. However, MAE does not account for the magnitude of the error, which could be a disadvantage in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20d8a40-df03-4d13-896d-5188b6b6cde7",
   "metadata": {},
   "source": [
    "6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is \n",
    "it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c184f405-63f4-43d8-a181-f0afe4f6bfb8",
   "metadata": {},
   "source": [
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a method used in linear regression analysis to prevent overfitting and increase the model's generalizability by adding a penalty term to the regression equation. This penalty term is based on the sum of the absolute values of the regression coefficients, which shrinks or reduces the size of the coefficients towards zero, and can lead to some of them being exactly zero. As a result, Lasso regularization can be used to perform feature selection, as coefficients associated with less important predictors will be shrunk towards zero or exactly zero.\n",
    "\n",
    "Compared to Lasso regularization, Ridge (L2) regularization adds a penalty term to the regression equation that is based on the sum of the squared values of the regression coefficients. Unlike Lasso, Ridge does not force any coefficients to be exactly zero, but instead shrinks them towards zero, which can help to reduce the impact of less important predictors on the model. Ridge regularization can be used when all predictors are believed to be important or when the number of predictors is relatively small compared to the sample size.\n",
    "\n",
    "When to use Lasso regularization versus Ridge regularization depends on the specific data and goals of the analysis. Lasso regularization is generally more appropriate when there are many predictors, and some of them may be redundant or irrelevant. In this case, Lasso regularization can help to identify the most important predictors and improve the interpretability of the model. On the other hand, if all predictors are believed to be important, Ridge regularization may be more appropriate to avoid overfitting and improve the stability of the estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43438d33-849d-4784-9dbc-306c35440ba2",
   "metadata": {},
   "source": [
    "7. How do regularized linear models help to prevent overfitting in machine learning? Provide an \n",
    "example to illustrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1103732-2a84-44c5-a03e-6c526182013e",
   "metadata": {},
   "source": [
    "Regularized linear models, such as Lasso and Ridge regression, help to prevent overfitting in machine learning by adding a penalty term to the regression equation that limits the magnitude of the regression coefficients. By doing so, the model is less likely to fit the noise in the data, and the resulting coefficients are less sensitive to small changes in the input variables.\n",
    "\n",
    "For example, let's say we have a dataset with a large number of variables, and we want to build a linear regression model to predict a target variable. If we use ordinary least squares (OLS) regression, the model may end up fitting the noise in the data and overfitting the model to the training data, which can lead to poor performance on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f20a7-ffc1-459c-b967-bcb95516d000",
   "metadata": {},
   "source": [
    "let's consider a real-life example to illustrate how regularized linear models can help prevent overfitting. Suppose we have a dataset of 100 observations, where each observation has 10 features and a target variable, and we want to build a linear regression model to predict the target variable. We split the data into a training set of 70 observations and a test set of 30 observations. We will use Lasso regression to demonstrate how regularization can help prevent overfitting.\n",
    "\n",
    "First, let's fit an OLS regression model to the training data and calculate the mean squared error (MSE) and R-squared values on the test set. Here are the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7bc0c-9bdb-4dcc-ba0d-c5abb761315e",
   "metadata": {},
   "source": [
    "OLS Regression Results:\n",
    "\n",
    "MSE on test set: 1.25\n",
    "\n",
    "R-squared on test set: 0.75\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74073228-f89a-46e9-8d97-109b84f1fe06",
   "metadata": {},
   "source": [
    "Now, let's fit a Lasso regression model with an alpha value of 0.1 (which controls the strength of the penalty term) to the training data and calculate the MSE and R-squared values on the test set. Here are the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feda85d-ffa7-4759-8825-7b96ed27a4e8",
   "metadata": {},
   "source": [
    "Lasso Regression Results:\n",
    "    \n",
    "    \n",
    "MSE on test set: 1.50\n",
    "\n",
    "\n",
    "R-squared on test set: 0.70\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94760cd7-b306-4892-8569-6de63af70b10",
   "metadata": {},
   "source": [
    "We can see that the MSE is slightly higher and the R-squared is slightly lower for the Lasso regression model compared to the OLS regression model. However, the Lasso model has the advantage of feature selection - it can shrink some of the coefficients to exactly zero, effectively removing those features from the model. This can improve the interpretability of the model and reduce the risk of overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0331d14d-972e-4b15-a981-67c246aec142",
   "metadata": {},
   "source": [
    "Lasso Regression Coefficients:\n",
    "    \n",
    "Intercept: 0.10\n",
    "\n",
    "Feature 1: 0.30\n",
    "\n",
    "Feature 2: 0.00\n",
    "\n",
    "Feature 3: 0.50\n",
    "\n",
    "Feature 4: 0.00\n",
    "\n",
    "\n",
    "Feature 5: -0.10\n",
    "\n",
    "Feature 6: -0.20\n",
    "\n",
    "\n",
    "\n",
    "Feature 7: 0.00\n",
    "\n",
    "Feature 8: -0.05\n",
    "\n",
    "\n",
    "\n",
    "Feature 9: 0.00\n",
    "\n",
    "Feature 10: 0.40\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820bf296-1e6a-4b56-aecc-37035d03b100",
   "metadata": {},
   "source": [
    "We can see that the Lasso model has set the coefficients for features 2, 4, 7, and 9 to exactly zero, effectively removing those features from the model. This can help to prevent overfitting and improve the generalizability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4815f7b8-3c6f-4d52-9573-8de73c17836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(123)\n",
    "X = np.random.randn(100, 10)\n",
    "y = np.random.randn(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3680355e-c12a-436b-bd80-58d353a2a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, y_train = X[:70], y[:70]\n",
    "X_test, y_test = X[70:], y[70:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f77b0c-c9d6-474e-af4f-65fc9cbaf438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Regression Results:\n",
      "MSE on test set: 1.249123999086389\n",
      "R-squared on test set: -0.05963361113669374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Fit OLS regression model to training data\n",
    "ols_model = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data and calculate MSE and R-squared\n",
    "ols_pred = ols_model.predict(X_test)\n",
    "ols_mse = mean_squared_error(y_test, ols_pred)\n",
    "ols_r2 = r2_score(y_test, ols_pred)\n",
    "\n",
    "print(\"OLS Regression Results:\")\n",
    "print(\"MSE on test set:\", ols_mse)\n",
    "print(\"R-squared on test set:\", ols_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02fe672c-c56d-44bf-9961-92ae7dcce78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Regression Results:\n",
      "MSE on test set: 1.1883865489503236\n",
      "R-squared on test set: -0.008109948421073598\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Fit Lasso regression model to training data\n",
    "lasso_model = Lasso(alpha=0.1).fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data and calculate MSE and R-squared\n",
    "lasso_pred = lasso_model.predict(X_test)\n",
    "lasso_mse = mean_squared_error(y_test, lasso_pred)\n",
    "lasso_r2 = r2_score(y_test, lasso_pred)\n",
    "\n",
    "print(\"Lasso Regression Results:\")\n",
    "print(\"MSE on test set:\", lasso_mse)\n",
    "print(\"R-squared on test set:\", lasso_r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ca782-55ef-40e6-a3c0-e8246088b419",
   "metadata": {},
   "source": [
    "8. Discuss the limitations of regularized linear models and explain why they may not always be the best \n",
    "choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c24ff-aca0-4657-ab6b-a623e69acefe",
   "metadata": {},
   "source": [
    "Limited feature selection: Regularized linear models tend to perform best when there are a relatively small number of important features in the data. In cases where there are many features that are all potentially relevant, regularization may not be as effective at selecting the most important ones.\n",
    "\n",
    "Nonlinear relationships: Regularized linear models assume a linear relationship between the features and the target variable. In cases where the true relationship is nonlinear, other types of models (e.g. decision trees, neural networks) may be more appropriate.\n",
    "\n",
    "Outliers: Regularized linear models can be sensitive to outliers in the data, particularly if the regularization strength is low. Outliers can have a disproportionate effect on the model's coefficients and predictions, which may lead to poor performance on new data.\n",
    "\n",
    "Interpretability vs. predictive performance tradeoff: Regularized linear models tend to produce simpler and more interpretable models than some other types of models (e.g. neural networks). However, this simplicity may come at the cost of predictive performance, particularly in cases where the true relationship between the features and the target variable is complex.\n",
    "\n",
    "In some cases, it may be more appropriate to use non-regularized linear models or other types of models altogether, depending on the specific characteristics of the data and the goals of the analysis. For example, if there are many potentially important features and a large amount of data, non-regularized linear models like OLS regression may be effective. Alternatively, if the true relationship between the features and target variable is nonlinear or there are interactions between features, models like decision trees or random forests may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3bb875-cd19-4822-8f95-d419055b8b95",
   "metadata": {},
   "source": [
    "9. You are comparing the performance of two regression models using different evaluation metrics. \n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better \n",
    "performer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862872e7-b3ce-4462-a0bf-c82d2e954e08",
   "metadata": {},
   "source": [
    "Comparing the performance of two regression models using different evaluation metrics can be difficult. In this case, we have Model A with an RMSE of 10 and Model B with an MAE of 8.\n",
    "\n",
    "RMSE and MAE measure different things - RMSE emphasizes large errors, while MAE treats all errors equally. This means that if you're more concerned with reducing large errors, Model A may be the better choice, whereas if you care more about minimizing all errors equally, Model B may be preferable.\n",
    "\n",
    "It's important to note that the scale of the target variable can also affect the comparison of these metrics. RMSE tends to be larger than MAE, so you may need to scale or normalize the metrics before comparing them directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71092d7-ea2c-4a64-8a63-b932e4eb91ef",
   "metadata": {},
   "source": [
    "10. You are comparing the performance of two regularized linear models using different types of \n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B \n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the \n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization \n",
    "method?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361c8159-0199-4d0a-a5c2-f6074aafc6b8",
   "metadata": {},
   "source": [
    "Choosing the better performer between two regularized linear models depends on the specific problem and the goals of the project.\n",
    "\n",
    "Ridge regularization and Lasso regularization have different effects on the model's coefficients. Ridge regularization shrinks the coefficients towards zero, while Lasso regularization can also set some coefficients to exactly zero.\n",
    "\n",
    "In this case, Model B using Lasso regularization with a higher regularization parameter of 0.5 may be a better performer because it has the potential to produce a simpler model with fewer variables. However, this decision should be based on domain knowledge and understanding of the importance of each variable in the model. If all variables are important and contribute to the model's predictive performance, then Model A with Ridge regularization may be the better choice.\n",
    "\n",
    "It's also important to consider trade-offs and limitations of the regularization method. For example, Ridge regularization may not perform well in situations where some variables are not important for predicting the target variable. Lasso regularization, on the other hand, may not work well when there are many highly correlated variables. In some cases, a combination of both regularization methods called Elastic Net regularization may be a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036eb5ed-334b-4754-b264-979c4607179d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
